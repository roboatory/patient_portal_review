{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76bfac6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rohit/.pyenv/versions/3.9.4/envs/data-science/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import emoji\n",
    "import fasttext\n",
    "from happytransformer import HappyTextToText, TTSettings\n",
    "import json\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "import spacy\n",
    "from spellchecker import SpellChecker\n",
    "import unicodedata\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cef62373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_case(dataset):\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d83aeaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unicode_normalization(dataset):\n",
    "    dataset[\"content\"] = dataset[\"content\"].apply(lambda s : unicodedata.normalize(\"NFKD\", s\n",
    "                                                        .encode(\"ascii\", \"ignore\").decode(\"utf-8\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b5365183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespace(dataset):\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"\\n\", \" \", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "283b1980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_quotations(dataset):\n",
    "    dataset = dataset.str.replace(\"’\", \"'\", regex = False)\n",
    "    dataset = dataset.str.replace(\"‘\", \"'\", regex = False)\n",
    "    dataset = dataset.str.replace(\"\\\"\", \"'\", regex = False)\n",
    "    dataset = dataset.str.replace(\"”\", \"'\", regex = False)\n",
    "    dataset = dataset.str.replace(\"“\", \"'\", regex = False)\n",
    "    dataset = dataset.str.replace(\"`\", \"'\", regex = False)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b15254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_contractions(dataset):\n",
    "    dataset[\"content\"] = correct_quotations(dataset[\"content\"])\n",
    "    \n",
    "    url = \"https://en.wikipedia.org/wiki/Wikipedia:List_of_English_contractions\"\n",
    "    \n",
    "    contractions_df = pd.read_html(url)[1]\n",
    "    contractions_df[\"Contraction\"] = contractions_df[\"Contraction\"].str.replace(\"(informal)\", \"\", regex = False)\n",
    "    contractions_df[\"Contraction\"] = contractions_df[\"Contraction\"].str.replace(\"(formal)\", \"\", regex = False)\n",
    "    contractions_df[\"Contraction\"] = contractions_df[\"Contraction\"].str.strip()\n",
    "    contractions_df[\"Full Form\"] = [re.search(r\"(.*?)[\\(,\\[]\", entry[0]).group()[:-1].strip() \n",
    "                                    if re.search(r\"(.*?)[\\(,\\[]\", entry[0]) else entry[0].strip()\n",
    "                                    for row, entry in enumerate(contractions_df[\"Full Form\"].str.split(\"/\"))]\n",
    "    \n",
    "    contractions_df.at[3, \"Contraction\"] = \"n\"\n",
    "    contractions_df.at[4, \"Full Form\"] = \"are not you\"\n",
    "    contractions_df.at[22, \"Full Form\"] = \"do not know\"\n",
    "    contractions_df.at[62, \"Full Form\"] = \"is not it\"\n",
    "    contractions_df.at[63, \"Full Form\"] = \"i do not\"\n",
    "    contractions_df.at[68, \"Full Form\"] = \"it is\"\n",
    "    contractions_df.at[69, \"Full Form\"] = \"i do not know\"\n",
    "    contractions_df.at[132, \"Full Form\"] = \"we all\"\n",
    "    contractions_df.at[181, \"Full Form\"] = \"yes madam\"\n",
    "    \n",
    "    contractions_df[\"Contraction\"] = correct_quotations(contractions_df[\"Contraction\"])\n",
    "    contractions_df[\"Full Form\"] = correct_quotations(contractions_df[\"Full Form\"])\n",
    "    \n",
    "    contractions = dict(zip(contractions_df[\"Contraction\"].str.lower(), \n",
    "                            contractions_df[\"Full Form\"].str.lower()))\n",
    "    \n",
    "    dataset[\"content\"] = dataset[\"content\"].apply(lambda s : [contractions[w] if w in contractions.keys() \n",
    "                                                              else w for w in s.split()]).str.join(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2009561a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis_to_text(dataset):\n",
    "    for index, review in enumerate(dataset[\"content\"]):\n",
    "        formatted_review = \"\"\n",
    "        for character in review:\n",
    "            if emoji.is_emoji(character):\n",
    "                formatted_review += \" \"\n",
    "            formatted_review += character\n",
    "        dataset.at[index, \"content\"] = formatted_review.strip()\n",
    "    \n",
    "    dataset[\"content\"] = dataset[\"content\"].apply(lambda s : emoji.demojize(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4156bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special characters across all datasets\n",
    "# {']', '*', '@', '%', '!', '/', ';', '&', \"'\", '$', '#', '~', '^', '}', '`', \n",
    "#  '\\\\', '.', '[', '(', '|', '<', ')', '+', '>', '=', ':', '{', '?', '-', ','}\n",
    "\n",
    "def filter_out_special_characters(dataset):\n",
    "    special_characters = set()\n",
    "    for review in dataset[\"content\"]:\n",
    "        special_characters.update(re.findall(r\"[^\\w\\s]\", review)) \n",
    "    for character in special_characters:\n",
    "        print(character + \" has {} occurrences\".format(dataset[\"content\"].apply(lambda s : \n",
    "                                                                                s.count(character)).sum()))\n",
    "         \n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(r\"[*#~^\\\\|<>=]\", \" \", regex = True)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"&\", \" and \", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"+\", \" plus \", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"@\", \" at \", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"/\", \" or \", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"%\", \" percent \", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"$\", \" dollar \", regex = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f4840e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_english_reviews(dataset, dataset_name):\n",
    "    language_model_path = \"models/lid.176.bin\"\n",
    "    language_model = fasttext.load_model(language_model_path)\n",
    "    \n",
    "    non_english_reviews = []\n",
    "    for index, review in enumerate(dataset[\"content\"]):\n",
    "        if len(review.split()) >= 5 and language_model.predict(review, k = 1)[0][0] != \"__label__en\":\n",
    "            non_english_reviews.append(index)\n",
    "        \n",
    "    print(\"List of non-english reviews: \" + str(non_english_reviews) + \"\\n\")\n",
    "    dataset.drop(non_english_reviews, axis = 0, inplace = True)\n",
    "    dataset.reset_index(drop = True, inplace = True)\n",
    "    \n",
    "    dataset.to_csv(\"current_datasets/english/\" + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "498bcea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_title(dataset):\n",
    "    for index, row in dataset.iterrows():\n",
    "        if (row[\"title\"][-1] == \".\" or \n",
    "            row[\"title\"][-1] == \"?\" or\n",
    "            row[\"title\"][-1] == \"!\"):\n",
    "            dataset.at[index, \"content\"] = row[\"title\"] + \" \" + row[\"content\"]\n",
    "        else:\n",
    "            dataset.at[index, \"content\"] = row[\"title\"] + \". \" + row[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ccb569f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_abbreviation(word):\n",
    "    abbreviations = {}\n",
    "    with open(\"current_datasets/spell/abbreviations.txt\", \"r\") as file:\n",
    "        abbreviations = json.loads(file.read())\n",
    "        \n",
    "    return (True, abbreviations[word]) if word in abbreviations else (False, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5939c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check_reviews(dataset, name):\n",
    "    spell = SpellChecker(distance = 2)\n",
    "    \n",
    "    def collect_word_information(time):\n",
    "        cleaned_reviews = dataset[\"content\"].str.replace(r\"[^a-zA-Z0-9\\s]\", \" \", regex = True)\n",
    "        overall_word_frequency = cleaned_reviews.str.split(expand = True).stack().value_counts()\n",
    "        overall_word_frequency = pd.DataFrame({\"Word\" : list(overall_word_frequency.index.values),\n",
    "                                               \"Posed Correction\" : [spell.correction(w) for w \n",
    "                                                                     in overall_word_frequency.index],\n",
    "                                               \"Frequency\" : list(overall_word_frequency.values)})\n",
    "        if time == \"before\":\n",
    "            overall_word_frequency.to_csv(\"current_datasets/spell/composite/\" + name)\n",
    "        \n",
    "        unknown_words = []\n",
    "        for _, row in overall_word_frequency.iterrows():\n",
    "            if (len(row[\"Word\"]) != 1 and row[\"Word\"][0].isalpha()\n",
    "                   and row[\"Word\"][-1].isalpha() and spell.unknown([row[\"Word\"]]) != set()):\n",
    "                unknown_words.append({\"Word\" : row[\"Word\"],\n",
    "                                      \"Posed Correction\" : row[\"Posed Correction\"],\n",
    "                                      \"Frequency\" : row[\"Frequency\"]})\n",
    "        \n",
    "        unknown_words = pd.DataFrame.from_records(unknown_words)\n",
    "        unknown_words.to_csv(\"current_datasets/spell/\" + time + \"/\" + name)\n",
    "        \n",
    "    collect_word_information(\"before\")\n",
    "    \n",
    "    spell.word_frequency.load_text_file('current_datasets/spell/words.txt')\n",
    "    \n",
    "    for df_index, review in enumerate(dataset[\"content\"]):\n",
    "        parsed_word_collection = []\n",
    "        for word in review.split():\n",
    "            word_fragment = \"\"\n",
    "            for character in word:\n",
    "                if character.isalnum():\n",
    "                    word_fragment += character\n",
    "                else:\n",
    "                    if word_fragment != \"\":\n",
    "                        parsed_word_collection.append(word_fragment)\n",
    "                        word_fragment = \"\"\n",
    "                    parsed_word_collection.append(character)\n",
    "                    \n",
    "            if word_fragment != \"\":\n",
    "                parsed_word_collection.append(word_fragment)\n",
    "        \n",
    "        for list_index, word in enumerate(parsed_word_collection):\n",
    "            if (len(word) != 1 and word[0].isalpha() \n",
    "                    and word[-1].isalpha() and word.isalnum()):\n",
    "                expansion = expand_abbreviation(parsed_word_collection[list_index])\n",
    "                proposed_correction = (spell.correction(parsed_word_collection[list_index]) \n",
    "                                           if not expansion[0] else expansion[1])\n",
    "                if proposed_correction:\n",
    "                    parsed_word_collection[list_index] = proposed_correction                \n",
    "\n",
    "        dataset.at[df_index, \"content\"] = \" \".join(parsed_word_collection)\n",
    "        \n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" .\", \".\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" ,\", \",\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" ?\", \"?\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" )\", \")\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"( \", \"(\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" !\", \"!\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" ;\", \";\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" :\", \":\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\" '\", \"'\", regex = False)\n",
    "    dataset[\"content\"] = dataset[\"content\"].str.replace(\"' \", \"'\", regex = False)\n",
    "    \n",
    "    collect_word_information(\"after\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435e7080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grammar_check_reviews(dataset):\n",
    "    happy_tt = HappyTextToText(\"T5\", \"vennify/t5-base-grammar-correction\")\n",
    "    args = TTSettings(num_beams = 5, min_length = 1)\n",
    "    for index, review in enumerate(dataset[\"content\"]):\n",
    "        result = happy_tt.generate_text(\"grammar: \" + review, args = args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55960423",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(cleaned_dataset, raw_dataset, name):\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "    \n",
    "    tokenized_columns = list(cleaned_dataset.columns)\n",
    "    tokenized_columns.extend([\"sentenceNumber\", \"sentence\", \"reviewNumber\"])\n",
    "    tokenized_dataset = pd.DataFrame(columns = tokenized_columns)\n",
    "    \n",
    "    for review_number, review in enumerate(cleaned_dataset[\"content\"]):\n",
    "        senter = [t for t in nlp(review).sents]\n",
    "        \n",
    "        encountered = 0\n",
    "        while encountered != len(senter):\n",
    "            if len(senter[encountered]) != 0:\n",
    "                review_metadata = raw_dataset.iloc[review_number].to_dict()\n",
    "                review_metadata[\"reviewNumber\"] = review_number + 1\n",
    "                review_metadata[\"sentenceNumber\"] = encountered + 1\n",
    "                review_metadata[\"sentence\"] = senter[encountered]\n",
    "                tokenized_dataset.loc[len(tokenized_dataset.index)] = review_metadata\n",
    "                \n",
    "            encountered += 1\n",
    "\n",
    "    tokenized_dataset.to_csv(\"current_datasets/tokenized/\" + name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "010e3b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def driver(apply_filter = None, sample = False, sample_size = 400):\n",
    "    files = apply_filter if apply_filter else os.listdir(\"current_datasets/raw\")\n",
    "    \n",
    "    for dataset_name in files:\n",
    "        dataset = pd.read_csv(\"current_datasets/raw/\" + dataset_name)\n",
    "        dataset = dataset.astype(str)\n",
    "        \n",
    "        if \"apple\" in dataset_name:\n",
    "            dataset.rename(columns = {\"review\" : \"content\",\n",
    "                                      \"body\" : \"developerBody\",\n",
    "                                      \"id\": \"developerID\",\n",
    "                                      \"modified\" : \"developerModified\"},\n",
    "                           inplace = True)\n",
    "            merge_title(dataset)\n",
    "        \n",
    "        print(\"\\nDataset name: \" + dataset_name + \"\\n\")\n",
    "        \n",
    "        standardize_case(dataset)\n",
    "        remove_whitespace(dataset)\n",
    "        keep_english_reviews(dataset, dataset_name)\n",
    "        \n",
    "        english_only_dataset = dataset.copy(deep = True)\n",
    "        expand_contractions(dataset)\n",
    "        spell_check_reviews(dataset, dataset_name)\n",
    "        convert_emojis_to_text(dataset)\n",
    "        unicode_normalization(dataset)\n",
    "        filter_out_special_characters(dataset)\n",
    "        tokenize_dataset(dataset, english_only_dataset, dataset_name)    \n",
    "    \n",
    "    if sample:\n",
    "        for dataset_name in files:\n",
    "            tokenized_dataset = pd.read_csv(\"current_datasets/tokenized/\" + dataset_name)\n",
    "            tokenized_dataset = tokenized_dataset.astype(str)\n",
    "            \n",
    "            np.random.seed(0)\n",
    "            sampled_review_numbers = np.random.choice(tokenized_dataset[\"reviewNumber\"].unique(), sample_size)\n",
    "            sampled_reviews = tokenized_dataset[tokenized_dataset[\"reviewNumber\"].isin(sampled_review_numbers)]\n",
    "            sampled_reviews.reset_index(inplace = True)\n",
    "            sampled_reviews.to_csv(\"current_datasets/sampled/\" + dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc9653a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver([\"MayoClinic_google.csv\"], True, 800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
